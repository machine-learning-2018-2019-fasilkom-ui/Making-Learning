{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Flatten\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "sources = [\"https://towardsdatascience.com/build-your-own-convolution-neural-network-in-5-mins-4217c2cf964f\", \"https://arxiv.org/pdf/1409.1556.pdf\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Image Dimension\n",
    "H = 128\n",
    "W = 128\n",
    "C = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CNN_EXP(num_classes):\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(32, kernel_size=(3, 3),\n",
    "                     activation='relu',\n",
    "                     input_shape=(H,W,1)))\n",
    "    model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(3, 3)))\n",
    "    model.add(Dropout(0.25))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(128, activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(num_classes, activation='softmax'))\n",
    "    model.compile(loss='sparse_categorical_crossentropy', optimizer='Adagrad',metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CNN_VGG_Lite(num_classes):\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(64, kernel_size=(3,3),padding='same',\n",
    "                     activation='relu', input_shape=(H,W,C)))\n",
    "    model.add(MaxPooling2D(pool_size=(3, 3), strides=(2,2)))\n",
    "    model.add(Conv2D(128, (3, 3), padding='same',activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(3, 3), strides=(2,2)))\n",
    "    model.add(Conv2D(256, (3, 3), padding='same',activation='relu'))\n",
    "    model.add(Conv2D(256, (3, 3), padding='same',activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(3, 3), strides=(2,2)))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(256, activation='relu'))\n",
    "    model.add(Dense(128, activation='relu'))\n",
    "    model.add(Dense(num_classes, activation='softmax'))\n",
    "    model.compile(loss='sparse_categorical_crossentropy', optimizer='sgd',metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# JAFFE utilization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_root_jaffe = 'dataset/jaffe/'\n",
    "image_title_jaffe = os.listdir(image_root)\n",
    "image_dirs_jaffe = [image_root + x for x in os.listdir(image_root)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AffectNet utilization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D:\\Documents\\KULIAH\\DSA\\TA\\Manually_Annotated\n"
     ]
    }
   ],
   "source": [
    "cd D:\\Documents\\KULIAH\\DSA\\TA\\Manually_Annotated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_csv = pd.read_csv('training.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(0: Neutral, 1: Happy, 2: Sad, 3:\n",
    "Surprise, 4: Fear, 5: Disgust, 6: Anger, 7: Contempt, 8: None, 9:\n",
    "Uncertain, 10: No-Face)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_csv = pd.read_csv('validation.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_root = 'Manually_Annotated_Images\\\\'\n",
    "image_dirs = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subDirectory_filePath</th>\n",
       "      <th>face_x</th>\n",
       "      <th>face_y</th>\n",
       "      <th>face_width</th>\n",
       "      <th>face_height</th>\n",
       "      <th>facial_landmarks</th>\n",
       "      <th>expression</th>\n",
       "      <th>valence</th>\n",
       "      <th>arousal</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>689/737db2483489148d783ef278f43f486c0a97e140fc...</td>\n",
       "      <td>134</td>\n",
       "      <td>134</td>\n",
       "      <td>899</td>\n",
       "      <td>899</td>\n",
       "      <td>181.64;530.91;188.32;627.82;195.1;723.37;205.2...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.785714</td>\n",
       "      <td>-0.055556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>392/c4db2f9b7e4b422d14b6e038f0cdc3ecee239b5532...</td>\n",
       "      <td>20</td>\n",
       "      <td>20</td>\n",
       "      <td>137</td>\n",
       "      <td>137</td>\n",
       "      <td>28.82;77.52;29.12;93.25;31.04;108.51;33.03;123...</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.017253</td>\n",
       "      <td>0.004313</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>468/21772b68dc8c2a11678c8739eca33adb6ccc658600...</td>\n",
       "      <td>11</td>\n",
       "      <td>11</td>\n",
       "      <td>176</td>\n",
       "      <td>176</td>\n",
       "      <td>30.52;87.33;32.55;106.43;36.94;125.81;43.06;14...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.174603</td>\n",
       "      <td>0.007937</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>944/06e9ae8d3b240eb68fa60534783eacafce2def60a8...</td>\n",
       "      <td>40</td>\n",
       "      <td>40</td>\n",
       "      <td>269</td>\n",
       "      <td>269</td>\n",
       "      <td>44.43;158.17;47.08;189.2;50.54;221.88;58.3;253...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.153401</td>\n",
       "      <td>0.038890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>993/02e06ee5521958b4042dd73abb444220609d96f57b...</td>\n",
       "      <td>22</td>\n",
       "      <td>22</td>\n",
       "      <td>153</td>\n",
       "      <td>153</td>\n",
       "      <td>50.59;78.72;48.6;93.23;48.72;109.06;48.8;123.0...</td>\n",
       "      <td>8</td>\n",
       "      <td>0.783972</td>\n",
       "      <td>-0.551684</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               subDirectory_filePath  face_x  face_y  \\\n",
       "0  689/737db2483489148d783ef278f43f486c0a97e140fc...     134     134   \n",
       "1  392/c4db2f9b7e4b422d14b6e038f0cdc3ecee239b5532...      20      20   \n",
       "2  468/21772b68dc8c2a11678c8739eca33adb6ccc658600...      11      11   \n",
       "3  944/06e9ae8d3b240eb68fa60534783eacafce2def60a8...      40      40   \n",
       "4  993/02e06ee5521958b4042dd73abb444220609d96f57b...      22      22   \n",
       "\n",
       "   face_width  face_height                                   facial_landmarks  \\\n",
       "0         899          899  181.64;530.91;188.32;627.82;195.1;723.37;205.2...   \n",
       "1         137          137  28.82;77.52;29.12;93.25;31.04;108.51;33.03;123...   \n",
       "2         176          176  30.52;87.33;32.55;106.43;36.94;125.81;43.06;14...   \n",
       "3         269          269  44.43;158.17;47.08;189.2;50.54;221.88;58.3;253...   \n",
       "4         153          153  50.59;78.72;48.6;93.23;48.72;109.06;48.8;123.0...   \n",
       "\n",
       "   expression   valence   arousal  \n",
       "0           1  0.785714 -0.055556  \n",
       "1           0 -0.017253  0.004313  \n",
       "2           0  0.174603  0.007937  \n",
       "3           1  0.153401  0.038890  \n",
       "4           8  0.783972 -0.551684  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_csv.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "size = 30000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_dirs = []\n",
    "for i in range(size):\n",
    "    path = image_root + training_csv['subDirectory_filePath'][i].replace('/','\\\\')\n",
    "    image_dirs.append(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_image(file_path):\n",
    "    img = cv2.imread(file_path, cv2.IMREAD_GRAYSCALE)\n",
    "    dimension = (H,W)\n",
    "    resized = cv2.resize(img, dimension)\n",
    "    return resized\n",
    "def prep_data(images):\n",
    "    count = len(images)\n",
    "    data = np.ndarray((count, C, H, W), dtype=np.uint8)\n",
    "    for i, image_file in enumerate(images):\n",
    "        image = read_image(image_file)\n",
    "        data[i] = image\n",
    "        if i%1000 == 0: print('Processed {} of {}'.format(i, count))\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 0 of 30000\n",
      "Processed 1000 of 30000\n",
      "Processed 2000 of 30000\n",
      "Processed 3000 of 30000\n",
      "Processed 4000 of 30000\n",
      "Processed 5000 of 30000\n",
      "Processed 6000 of 30000\n",
      "Processed 7000 of 30000\n",
      "Processed 8000 of 30000\n",
      "Processed 9000 of 30000\n",
      "Processed 10000 of 30000\n",
      "Processed 11000 of 30000\n",
      "Processed 12000 of 30000\n",
      "Processed 13000 of 30000\n",
      "Processed 14000 of 30000\n",
      "Processed 15000 of 30000\n",
      "Processed 16000 of 30000\n",
      "Processed 17000 of 30000\n",
      "Processed 18000 of 30000\n",
      "Processed 19000 of 30000\n",
      "Processed 20000 of 30000\n",
      "Processed 21000 of 30000\n",
      "Processed 22000 of 30000\n",
      "Processed 23000 of 30000\n",
      "Processed 24000 of 30000\n",
      "Processed 25000 of 30000\n",
      "Processed 26000 of 30000\n",
      "Processed 27000 of 30000\n",
      "Processed 28000 of 30000\n",
      "Processed 29000 of 30000\n"
     ]
    }
   ],
   "source": [
    "images = prep_data(image_dirs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for AffectNet\n",
    "labels = training_csv['expression'][:size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For jaffe\n",
    "image_labels_jaffe = np.array([title[3:5] for title in image_title])\n",
    "labels_jaffe = []\n",
    "for i, label in enumerate(image_labels_jaffe):\n",
    "    if label == 'AN':\n",
    "        labels_jaffe.append(0)\n",
    "    if label == 'DI':\n",
    "        labels_jaffe.append(1)\n",
    "    if label == 'FE':\n",
    "        labels_jaffe.append(2)\n",
    "    if label == 'SU':\n",
    "        labels_jaffe.append(3)\n",
    "    if label == 'SA':\n",
    "        labels_jaffe.append(4)\n",
    "    if label == 'HA':\n",
    "        labels_jaffe.append(5)\n",
    "    if label == 'NE':\n",
    "        labels_jaffe.append(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "X_train = images[:8000]\n",
    "X_test = images[8000:10000]\n",
    "\n",
    "X_train = X_train.reshape(X_train.shape[0], H, W, C)\n",
    "X_test = X_test.reshape(X_test.shape[0], H, W, C)\n",
    "\n",
    "\n",
    "y_train = labels[:8000]\n",
    "y_test = labels[8000:10000]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_exp = CNN_EXP(11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_8 (Conv2D)            (None, 126, 126, 32)      320       \n",
      "_________________________________________________________________\n",
      "conv2d_9 (Conv2D)            (None, 124, 124, 64)      18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_5 (MaxPooling2 (None, 41, 41, 64)        0         \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 41, 41, 64)        0         \n",
      "_________________________________________________________________\n",
      "flatten_3 (Flatten)          (None, 107584)            0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 128)               13770880  \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 11)                1419      \n",
      "=================================================================\n",
      "Total params: 13,791,115\n",
      "Trainable params: 13,791,115\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "cnn_exp.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 8000 samples, validate on 2000 samples\n",
      "Epoch 1/10\n",
      " - 23s - loss: 11.2034 - acc: 0.3047 - val_loss: 11.0490 - val_acc: 0.3145\n",
      "Epoch 2/10\n",
      " - 23s - loss: 11.1275 - acc: 0.3096 - val_loss: 11.0490 - val_acc: 0.3145\n",
      "Epoch 3/10\n",
      " - 23s - loss: 10.9805 - acc: 0.3187 - val_loss: 11.0490 - val_acc: 0.3145\n",
      "Epoch 4/10\n",
      " - 23s - loss: 10.9704 - acc: 0.3194 - val_loss: 11.0490 - val_acc: 0.3145\n",
      "Epoch 5/10\n",
      " - 23s - loss: 11.0454 - acc: 0.3146 - val_loss: 11.0490 - val_acc: 0.3145\n",
      "Epoch 6/10\n",
      " - 23s - loss: 11.6937 - acc: 0.2745 - val_loss: 11.0490 - val_acc: 0.3145\n",
      "Epoch 7/10\n",
      " - 23s - loss: 11.7984 - acc: 0.2680 - val_loss: 11.0490 - val_acc: 0.3145\n",
      "Epoch 8/10\n",
      " - 23s - loss: 11.5929 - acc: 0.2808 - val_loss: 11.0490 - val_acc: 0.3145\n",
      "Epoch 9/10\n",
      " - 23s - loss: 11.5667 - acc: 0.2824 - val_loss: 11.0490 - val_acc: 0.3145\n",
      "Epoch 10/10\n",
      " - 23s - loss: 11.6332 - acc: 0.2783 - val_loss: 11.0490 - val_acc: 0.3145\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x245e746b6a0>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnn_exp.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=10,batch_size=8, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Error: 68.55%\n"
     ]
    }
   ],
   "source": [
    "scores = cnn_exp.evaluate(X_test, y_test, verbose=0)\n",
    "print(\"Classification Error: %.2f%%\" % (100-scores[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 128, 128, 64)      640       \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 63, 63, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 63, 63, 128)       73856     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 31, 31, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 31, 31, 256)       295168    \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 31, 31, 256)       590080    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 15, 15, 256)       0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 57600)             0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 256)               14745856  \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 128)               32896     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 11)                1419      \n",
      "=================================================================\n",
      "Total params: 15,739,915\n",
      "Trainable params: 15,739,915\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_vgg_lite.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 28000 samples, validate on 2000 samples\n",
      "Epoch 1/20\n",
      " - 106s - loss: 13.1882 - acc: 0.1817 - val_loss: 13.6117 - val_acc: 0.1555\n",
      "Epoch 2/20\n",
      " - 91s - loss: 13.1886 - acc: 0.1817 - val_loss: 13.6117 - val_acc: 0.1555\n",
      "Epoch 3/20\n",
      " - 91s - loss: 13.1886 - acc: 0.1817 - val_loss: 13.6117 - val_acc: 0.1555\n",
      "Epoch 4/20\n",
      " - 91s - loss: 13.1886 - acc: 0.1817 - val_loss: 13.6117 - val_acc: 0.1555\n",
      "Epoch 5/20\n",
      " - 91s - loss: 13.1886 - acc: 0.1817 - val_loss: 13.6117 - val_acc: 0.1555\n",
      "Epoch 6/20\n",
      " - 91s - loss: 13.1886 - acc: 0.1817 - val_loss: 13.6117 - val_acc: 0.1555\n",
      "Epoch 7/20\n",
      " - 91s - loss: 13.1886 - acc: 0.1817 - val_loss: 13.6117 - val_acc: 0.1555\n",
      "Epoch 8/20\n",
      " - 91s - loss: 13.1886 - acc: 0.1817 - val_loss: 13.6117 - val_acc: 0.1555\n",
      "Epoch 9/20\n",
      " - 91s - loss: 13.1886 - acc: 0.1817 - val_loss: 13.6117 - val_acc: 0.1555\n",
      "Epoch 10/20\n",
      " - 91s - loss: 13.1886 - acc: 0.1817 - val_loss: 13.6117 - val_acc: 0.1555\n",
      "Epoch 11/20\n",
      " - 91s - loss: 13.1886 - acc: 0.1817 - val_loss: 13.6117 - val_acc: 0.1555\n",
      "Epoch 12/20\n",
      " - 91s - loss: 13.1886 - acc: 0.1817 - val_loss: 13.6117 - val_acc: 0.1555\n",
      "Epoch 13/20\n",
      " - 91s - loss: 13.1886 - acc: 0.1817 - val_loss: 13.6117 - val_acc: 0.1555\n",
      "Epoch 14/20\n",
      " - 91s - loss: 13.1886 - acc: 0.1817 - val_loss: 13.6117 - val_acc: 0.1555\n",
      "Epoch 15/20\n",
      " - 91s - loss: 13.1886 - acc: 0.1817 - val_loss: 13.6117 - val_acc: 0.1555\n",
      "Epoch 16/20\n",
      " - 91s - loss: 13.1886 - acc: 0.1817 - val_loss: 13.6117 - val_acc: 0.1555\n",
      "Epoch 17/20\n",
      " - 91s - loss: 13.1886 - acc: 0.1817 - val_loss: 13.6117 - val_acc: 0.1555\n",
      "Epoch 18/20\n",
      " - 91s - loss: 13.1886 - acc: 0.1817 - val_loss: 13.6117 - val_acc: 0.1555\n",
      "Epoch 19/20\n",
      " - 91s - loss: 13.1886 - acc: 0.1817 - val_loss: 13.6117 - val_acc: 0.1555\n",
      "Epoch 20/20\n",
      " - 91s - loss: 13.1886 - acc: 0.1817 - val_loss: 13.6117 - val_acc: 0.1555\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x2442b218eb8>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_vgg_lite.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=20,batch_size=8, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Error: 84.45%\n"
     ]
    }
   ],
   "source": [
    "scores = model_vgg_lite.evaluate(X_test, y_test, verbose=0)\n",
    "print(\"Classification Error: %.2f%%\" % (100-scores[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From D:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    }
   ],
   "source": [
    "model_vgg_lite = CNN_VGG_Lite(11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 8000 samples, validate on 2000 samples\n",
      "Epoch 1/10\n",
      " - 17s - loss: 11.5869 - acc: 0.2811 - val_loss: 11.0490 - val_acc: 0.3145\n",
      "Epoch 2/10\n",
      " - 17s - loss: 12.6525 - acc: 0.2149 - val_loss: 13.0234 - val_acc: 0.1920\n",
      "Epoch 3/10\n",
      " - 17s - loss: 12.8351 - acc: 0.2036 - val_loss: 13.0234 - val_acc: 0.1920\n",
      "Epoch 4/10\n",
      " - 17s - loss: 12.7131 - acc: 0.2113 - val_loss: 13.0234 - val_acc: 0.1920\n",
      "Epoch 5/10\n",
      " - 17s - loss: 12.6406 - acc: 0.2157 - val_loss: 13.0234 - val_acc: 0.1920\n",
      "Epoch 6/10\n",
      " - 17s - loss: 12.7796 - acc: 0.2071 - val_loss: 13.0234 - val_acc: 0.1920\n",
      "Epoch 7/10\n",
      " - 17s - loss: 12.6628 - acc: 0.2144 - val_loss: 13.0234 - val_acc: 0.1920\n",
      "Epoch 8/10\n",
      " - 17s - loss: 12.7655 - acc: 0.2080 - val_loss: 13.0234 - val_acc: 0.1920\n",
      "Epoch 9/10\n",
      " - 17s - loss: 12.7172 - acc: 0.2110 - val_loss: 13.0234 - val_acc: 0.1920\n",
      "Epoch 10/10\n",
      " - 17s - loss: 12.7112 - acc: 0.2114 - val_loss: 13.0234 - val_acc: 0.1920\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x245e7483080>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnn_exp.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=10,batch_size=15, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Error: 80.80%\n"
     ]
    }
   ],
   "source": [
    "scores = cnn_exp.evaluate(X_test, y_test, verbose=0)\n",
    "print(\"Classification Error: %.2f%%\" % (100-scores[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
